{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI9TCKmFSCn69T8WUmMSB/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI-4967-Projects-in-ML-AI/blob/main/Lecture_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 15\n",
        "\n",
        "# Topics for today\n",
        "\n",
        "1. Introduction to Natural Language Processing(NLP)\n",
        "2. Introduction to Language Models\n",
        "3. Text Classification"
      ],
      "metadata": {
        "id": "fB5wIGt93JNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Any task involving machines dealing with human language falls under the umbrella of NLP. Tasks such as translating between languages,  text analysis, speech recognition and automatic text generation all are common NLP taks. NLP finds applications in voice assistants like Siri and Alexa, and even search engines such as Google and Bing."
      ],
      "metadata": {
        "id": "wATsjgao3elW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary\n",
        "1. Text Corpus: the text corpus refers to the set of texts used for the task. For example, if we were building a model to analyze news articles, our text corpus would be the entire set of articles or papers we used to train and evaluate the model.\n",
        "\n",
        "2. Tokenization: A piece of text can be represented as a vector/list of its vocabulary words. This process is known as tokenization, where each individual vocabulary word in a piece of text is a token.\n",
        "\n",
        "3. Embeddings: Vector representation for words in vocabulary. Examples include Word2vec, Glove etc.The basis for embedding vectors comes from the concept of a target word and its context window. For each word of each tokenized sequence in the text corpus, we treat it as a target, and the words around it as the context window.\n",
        "\n",
        "4. Candidate Sampling: Training an embedding model is equivalent to multiclass classification, where the possible classes include every single vocabulary word. In order to mitigate the full softmax operation, we apply something called candidate sampling. Under candidate sampling, we choose a much smaller fraction of the possible classes (i.e. vocabulary words) for computing the loss."
      ],
      "metadata": {
        "id": "lBp-C5wV3_Wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Functions\n",
        "1. Sampled softmax loss\n",
        "\n",
        "[Link](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss)\n",
        "\n",
        "2. Noise contrastive estimation (NCE): Covert multiclass classification problem to binary classification problem.\n",
        "\n",
        "[Link](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss)"
      ],
      "metadata": {
        "id": "F0bEkJXaJh6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Probabilities\n",
        "\n",
        "The goal of any language model is to assign probabilities to words in a given text sequence. This is a multiclass classification problem sicne each word can be considered a class with the words appearing before it as input.\n",
        " Original sentence = ['She', 'went', 'to', 'buy','a','book']\n",
        "\n",
        " Input =  ['She', 'went', 'to', 'buy','a']\n",
        "\n",
        " Target = ['went', 'to', 'buy','a','book']\n",
        "\n",
        " The language model attempts to predict each word in the target sequence based on its corresponding prefix in the input. In this example we can have the following pairs of input-target sequences. (she,went),((she,went),to) and so on.\n",
        "We can choose to fix our training set sequences to a given maximum length."
      ],
      "metadata": {
        "id": "TEAYMUkEj0gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Language Models\n",
        "\n",
        "We have discussed RNNs, LSTMS, GRUs etc. These are neural network architectures for sequential data.\n",
        "\n",
        "[Link](Project)"
      ],
      "metadata": {
        "id": "Mcf8AwUtItqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq Framework\n",
        "\n",
        "One of the popular frameworks in NLP is the sequence to sequence (seq2seq) framework. This framework involves any task that takes in some text and returns some generated text. Dialog systems (e.g. chatbots), text summarization, and machine translation are all seq2seq applications.\n",
        "\n",
        "Training involves the following tasks:\n",
        "\n",
        "1. Input Task: Extract meaningful information from a given input.\n",
        "\n",
        "2. Output Task: Calculate probabilities of words for the output sequence. This is referred to as language modelling where we process the ground truth sequence with the final token sequence.\n",
        "\n",
        "For seq2seq models, it is important to have start-of-sequence (SOS) and end-of-sequence (EOS) tokens. These tokens mark the start and end of a tokenized text sequence."
      ],
      "metadata": {
        "id": "AYFGeAZUu21G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We learnt about LSTM model and its variants e.g. BiLSTM. In an ecoder-decoder model there are two components: encoder and decoder. For NLP applications encoder is LSTM/BiLSTM.\n",
        "\n",
        "LSTM final state is important because that is the information passed on to the decoder. In TensorFlow it's represented by the LSTMStateTuple object.\n",
        "[Link](https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/rnn_cell/LSTMStateTuple)\n",
        "\n",
        "When the BiLSTM has multiple layers, the final_states output of tf.keras.layers.Bidirectional is a tuple containing the final forward and backward states for each layer. In order to use BiLSTM final states in an encoder-decoder model, we need to combine the forward and backward states. This is because the decoder portion utilizes a regular LSTM, which only has a forward direction."
      ],
      "metadata": {
        "id": "OyzqDc1MHLHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Decoder Architecture\n",
        "\n",
        "What makes an encoder-decoder model so powerful is that the decoder uses the final state of the encoder as its initial state. This gives the decoder access to the information that the encoder extracted from the input sequence, which is crucial for good sequence to sequence modeling.\n",
        "\n",
        "In the case of multiple layers for the encoder-decoder model, each layer's final state in the encoder acts as the initial state for the corresponding layer in the decoder.\n",
        "\n",
        "[Reading](https://pradeep-dhote9.medium.com/seq2seq-encoder-decoder-lstm-model-1a1c9a43bbac)\n"
      ],
      "metadata": {
        "id": "7lLsFXfKJTbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Mechanisms\n",
        "\n",
        "In general, long-term dependencies cannot be captured by the final state of the encoder only. Sometimes, the decoder needs access to intermediate states of the encoder to completely model long tem dependencies.\n",
        "\n",
        "Attention mechanism lets the decoder decide which encoder outputs are most useful for the decoder at the current time step.Using the decoder's hidden state at the current time step, as well as the encoder outputs, attention will calculate something called a context vector. The context vector encapsulates the most meaningful information from the input sequence for the current decoder time step, and it's used as additional input for the decoder when calculating the time step's output.\n",
        "\n",
        "Attention makes use of trainable weights to calculate the context vector. There are two most popular techniques for calculating the context vextor - in Tensorflow these are BahdanauAttention and LuongAttention. These two attention mechanisms are named after their main inventors, Dzmitry Bahdanau and Minh-Thang Luong, respectively.\n",
        "The main difference between the two mechanisms is how they combine the encoder outputs and current time step hidden state when computing the context vector. The Bahdanau mechanism uses an additive (concatenation-based) method, while the Luong mechanism uses a multiplicative method.\n",
        "\n",
        "[Link](https://www.tensorflow.org/text/tutorials/nmt_with_attention)\n",
        "\n",
        "[Paper 1](https://arxiv.org/pdf/1409.0473.pdf)"
      ],
      "metadata": {
        "id": "MZ2UqD9kMvss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers\n",
        "\n",
        "A transformer is a deep learning architecture developed by Google with its roots in multi-head attention mechanism. It does not include recurrent units and requires less training time. Transformers are the the key to large language models (LLMs).\n",
        "\n",
        "[Paper 2](https://arxiv.org/pdf/1706.03762.pdf)"
      ],
      "metadata": {
        "id": "E2Dgw8c6R_TJ"
      }
    }
  ]
}