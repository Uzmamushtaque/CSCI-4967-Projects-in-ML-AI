{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObGHmFqKZATIPEIYLoj6eU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI-4967-Projects-in-ML-AI/blob/main/Lecture_22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture 22\n",
        "\n",
        "### Topics for Today\n",
        "\n",
        "Recent Research Trend in Time series research."
      ],
      "metadata": {
        "id": "kT62ztDbEZDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        " In the previous lecture we discussed statistical models for Time series analysis.Deep learning sequential models have also been discussed previously. Today's lecture is dedicated to the state-of-the-art research in this area."
      ],
      "metadata": {
        "id": "0row_26KInVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Papers\n",
        "\n",
        "1. [Link](https://arxiv.org/pdf/1912.09363v3.pdf): The paper proposes a new absolute position encoding method called time Absolute Position Encoding (tAPE) and a computationally efficient implementation of Relative Position Encoding (eRPE) for time series data.\n",
        "The authors introduce a novel multivariate time series classification (MTSC) model called ConvTran, which combines tAPE/eRPE with convolution-based input encoding.\n",
        "Extensive experiments on 32 multivariate time-series datasets demonstrate that the proposed model outperforms state-of-the-art convolution and transformer-based models in terms of accuracy.\n",
        "The proposed absolute and relative position encoding methods are simple and efficient, and can be easily integrated into transformer blocks for various downstream tasks such as forecasting, extrinsic regression, and anomaly detection.\n",
        "The code and models for the proposed methods are open-sourced and available on GitHub.\n",
        "\n",
        "2. [Link](https://arxiv.org/pdf/2403.09809v1.pdf): Self-supervised learning (SSL) for time series data encompasses both contrastive and generative approaches. Contrastive-based methods, as highlighted in [1], have shown significant promise by reducing the reliance on labeled data through pretraining on unlabeled datasets. These methods, such as the TS2Vec loss, have demonstrated superior performance in various time series tasks. On the other hand, generative-based SSL methods, as discussed in [2], also play a crucial role in enhancing representation learning for time series data. By providing a comprehensive taxonomy of SSL methods for time series analysis, researchers aim to bridge the gap in understanding the impact of different strategies on time series forecasting performance. Both contrastive and generative approaches contribute to the advancement of SSL in time series tasks, offering valuable insights for future research endeavors.\n",
        "\n",
        "3. [Link](https://paperswithcode.com/paper/exploring-the-influence-of-dimensionality): The multi-scale convolutional recurrent variational autoencoder (MSCRVAE) model integrates dimensionality reduction, enhancing anomaly detection in multivariate time series with superior F1-scores up to 0.90.\n",
        "\n",
        "4. [Link](https://arxiv.org/pdf/2403.07815.pdf): Chronos, a family of pretrained time series models based on language model architectures. Like large language models or vision-language models, Chronos is a foundation model, which learns from large datasets how to produce general representations useful for a wide range of tasks.\n",
        "\n",
        "5. [Link](https://arxiv.org/pdf/2402.02713v1.pdf)\n",
        "\n",
        "6. [Link](https://arxiv.org/pdf/2402.02713v1.pdf)\n",
        "\n",
        "7. [Link](https://arxiv.org/pdf/2310.03589v1.pdf)\n",
        "\n",
        "8. [Link](https://arxiv.org/pdf/2305.16642v1.pdf)"
      ],
      "metadata": {
        "id": "eh-gmy_AGcaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Link to other relevant research](https://paperswithcode.com/sota)"
      ],
      "metadata": {
        "id": "8BACD2w44i62"
      }
    }
  ]
}