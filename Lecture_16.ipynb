{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz+ss0XHJsyaX6Bsk3BAmd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI-4967-Projects-in-ML-AI/blob/main/Lecture_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 16\n",
        "\n",
        "## Today's Topics\n",
        "\n",
        "1. Large Language Models (LLMs)\n",
        "2. Components of LLMs\n",
        "3. Capabilities and Applications\n",
        "4. Limitations"
      ],
      "metadata": {
        "id": "ytQZ0r_gSnRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "An LLM is a probabilistic model that assigns probability values to sequences of words or tokens in a given language. The goal is to capture the underlying pattern in the data to predict the likelihood of a sequence of words.\n",
        "The LLms of today are advanced models trained on massive amounts of data and are designed to generate human like text.\n"
      ],
      "metadata": {
        "id": "vnD6eDSPS5_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Components of LLMs\n",
        "\n",
        "Most of the recent LLMs are built on the Transformer architecture. At the heart of the Transformer are two fundamental principles: the use of self-attention mechanisms and an encoder-decoder structure.\n",
        "\n",
        "For more details refer to Lecture 15 material."
      ],
      "metadata": {
        "id": "kz8yogX6V-Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of LLMs\n",
        "\n",
        "1. Language Representation Models: These models have a bidirectional context understanding. These are versatile because they can be used for various downstream tasks\n",
        "\n",
        "[Reading](https://www.mdpi.com/1099-4300/23/11/1422#:~:text=The%20basis%20for%20each%20neural,are%20usually%20called%20word%20embeddings.)\n",
        "\n",
        "\n",
        "2. Zero shot learning models: Zero-shot learning (ZSL) is a machine learning technique that allows pre-trained models to predict class labels for data samples that aren't present in the training data. ZSL methods work by using auxiliary information to associate observed and non-observed classes. This information encodes observable distinguishing properties of objects. The GPT series of language models ahve this generative capability as these models have been trained on diverse datasets.\n",
        "\n",
        "3. Multi-shot learning models: These models excel in adapting to tasks with few examples. The essence of multi-shot learning lies in providing the model with limited examples for a specific task, allowing it to perform well with minimal training.\n",
        "\n",
        "[Reading](https://www.analyticsvidhya.com/blog/2022/12/know-about-zero-shot-one-shot-and-few-shot-learning/)\n",
        "\n",
        "4. Fine Tuned or domain specific models: These reperesent a category of models that undergo additional training tailored to a specific task or domain. Fine-tuning enables these models to adapt to the intricacies of specific tasks or domains, resulting in improved performance and task-specific expertise."
      ],
      "metadata": {
        "id": "vZR2VLfsYSYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters in some of the recent LLMs:\n",
        "\n",
        "1. GPT4: 1.7 trillion parameters\n",
        "2. Llama: 65 billion\n",
        "3. BARD: 137 billion parameters"
      ],
      "metadata": {
        "id": "PA7SwYJCdWjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Capabilities\n",
        "\n",
        "1. Language Modeling:A language model is a type of machine learning model trained to conduct a probability distribution over words. Put it simply, a model tries to predict the next most appropriate word to fill in a blank space in a sentence or phrase, based on the context of the given text.\n",
        "[Link](https://www.altexsoft.com/blog/language-models-gpt/)\n",
        "Evaluation Metric: Perplexity\n",
        "Note: The test perplexity value of GPT-3 surpasses all other language models created before July 2020 with a score of 20.5. Also note that in most of the other tasks, GPT-3 beats other SOTA models.\n",
        "2. Question Answering: Question answering involves providing a model with a question and a context (such as a passage of text), and the model generates an accurate and relevant answer based on its understanding of the content.\n",
        "Accuracy is the metric to measure how well the model answered all the questions.\n",
        "3. Translations: Translation involves converting text from one language to another while preserving the meaning.\n",
        "Bilingual Evaluation Understudy (BLEU) measures the quality of machine-translated text by comparing it to one or more human reference translations.\n",
        "ROUGE is a set of metrics that compare machine-generated texts to a set of references (typically human-generated). The main focus of ROUGE is to measure the overlap of n-grams, word sequences, and word pairs between the machine-generated and the reference texts.\n",
        "4. Arithmetic Problem Solving: Arithmetic tasks involve performing mathematical operations using natural language input, such as addition, subtraction, multiplication, or division. The model understands the numerical and symbolic representations and provides the correct result.\n",
        "5. Text Generation: Generating related to a specific topic or area. BLEU score is the most common evaluation metric.\n",
        "\n",
        "[Link](https://towardsdatascience.com/how-to-evaluate-text-generation-models-metrics-for-automatic-evaluation-of-nlp-models-e1c251b04ec1)"
      ],
      "metadata": {
        "id": "FuSRN7WTeUcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning an LLM\n",
        "\n",
        "Fine tuning is the process of adjusting paramters of a pretrained LLM on a specific dataset to increase its performance on a specific task. For example the first version of GPT-3 was trained on a huge and diverse dataset for text completion.These models could accurately predict the next word in a sentence. However, as they were not trained to specifically follow instructions from users, these models had the potential of generating inaccurate or misleading outputs unrelated to the usersâ€™ instructions.\n",
        "\n",
        "The researchers at OpenAI fine-tuned GPT-3 on a prompt-based dataset to make the models safer and more helpful for their users. This fine-tuning resulted in the InstructGPT models that excel at following user prompts and generating outputs more aligned with the given instructions.\n",
        "\n",
        "In general fine-tuning requires an iterative training scheme such that the existing paramters of the model are updated as per the new datasets.\n",
        "Examples of fine tuned models: ChatGPT (Fine tuned for conversational text),\n",
        "Koala (chatbot fine tuned for academic research): [Link](https://bair.berkeley.edu/blog/2023/04/03/koala/) , Starcoder(model fine-tuned on a diverse and extensive code dataset from GitHub, which includes a variety of programming languages and related content such as Git commits, GitHub issues, and Jupyter notebooks), [Link](https://github.com/bigcode-project/starcoder)\n",
        "\n"
      ],
      "metadata": {
        "id": "6P6KXYMTjDWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection\n",
        "\n",
        "Criteria for model selection:\n",
        "\n",
        "1. Model Size: Model size impacts model performance but larger models require more computational power. Depending on our reuirements we may opt for a smaller model like GPT-2, that has 124 million parameters or choose a more powerful option like Llama-2, which has 70 billion parameters and provides a higher level of performance.\n",
        "\n",
        "2. Pretraining: A knowledge of the pretraining of the LLM greatly influences our understanding of user prompts and outputs. Any model trained on internet data will be diverse e.g. Common Crawl was used to train GPT-3, Llama. This will also give us a good understanding of the limitation of the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "BA19T1PHirpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing LLMs\n",
        "\n",
        "1. GitHub\n",
        "\n",
        "2. Hugging Face: It includes an expansive repository for machine learning models and is particularly known for its collection of pretrained LLMs."
      ],
      "metadata": {
        "id": "cnuZ5_kKocds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Papers\n",
        "\n",
        "[1](https://arxiv.org/pdf/2307.09288.pdf%C3%82%C2%A0)\n",
        "\n",
        "[2](https://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf)\n"
      ],
      "metadata": {
        "id": "d_xXeLH2pTUY"
      }
    }
  ]
}