{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwKasQqzzWFJkT3i1a0hPb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI-4967-Projects-in-ML-AI/blob/main/Lecture_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Today's Lecture\n",
        "\n",
        "1. More about ensemble learning\n",
        "2. Important Boosting Agorithms\n",
        "3. XGBoost\n",
        "4. Important Research in the field of ensemle learning"
      ],
      "metadata": {
        "id": "lQrwZnZZ9e5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tree based Methods are extensively used in various applications across all domains. Today's lecture is aimed at emphasizing the techniques that are proven to give good results in most practical settings."
      ],
      "metadata": {
        "id": "U4gZRQs6N2yo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting Overview\n",
        "\n",
        "Boosting is a technique through which multiple weak learners are combined adaptively to make accurate predictions. To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
        "\n",
        "[Source](https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/)"
      ],
      "metadata": {
        "id": "gHV09qYC9tdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of Boosting Algorithms\n",
        "\n",
        "1. AdaBoost (Adaptive Boosting)\n",
        "\n",
        "2. Gradient Tree Boosting\n",
        "\n",
        "3. XGBoost"
      ],
      "metadata": {
        "id": "2KJLPriXKuoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost\n",
        "\n",
        "* Build a model and make predictions.\n",
        "\n",
        "* Assign higher weights to miss-classified points.\n",
        "\n",
        "* Build next model.\n",
        "\n",
        "* Repeat steps 3 and 4.\n",
        "\n",
        "* Make a final model using the weighted average of individual models.\n",
        "\n",
        "[Link1](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-adaboost-algorithm-with-python-implementation/)\n",
        "\n",
        "[Link2](https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/#:~:text=AdaBoost%20algorithm%2C%20short%20for%20Adaptive,assigned%20to%20incorrectly%20classified%20instances.)\n"
      ],
      "metadata": {
        "id": "Bz94A3jNNTfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting\n",
        "\n",
        "Gradient boosting is a machine learning ensemble technique that combines the predictions of multiple weak learners, typically decision trees, sequentially. It aims to improve overall predictive performance by optimizing the model’s weights based on the errors of previous iterations, gradually reducing prediction errors and enhancing the model’s accuracy.\n",
        "\n",
        "[Source](https://www.analyticsvidhya.com/blog/2021/09/gradient-boosting-algorithm-a-complete-guide-for-beginners/)"
      ],
      "metadata": {
        "id": "VauRw-yMNv1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost\n",
        "\n",
        "XGBoost is a popular implementation of gradient boosting. Let’s discuss some features of XGBoost that make it so interesting:\n",
        "\n",
        "**Regularization:**  Regularization helps in preventing overfitting. XGBoost uses an option to incorporate both L1 and L2 regularization.\n",
        "\n",
        "**Handling sparse data:** Pre-processing steps like missing values imputation or one-hot encoding make data sparse. XGBoost incorporates a sparsity-aware split finding algorithm to handle different types of sparsity patterns in the data\n",
        "\n",
        "**Weighted quantile sketch:** Most existing tree based algorithms can find the split points when the data points are of equal weights (using quantile sketch algorithm). However, they are not equipped to handle weighted data. XGBoost has a distributed weighted quantile sketch algorithm to effectively handle weighted data\n",
        "\n",
        "**Block structure for parallel learning:** For faster computing, XGBoost can make use of multiple cores on the CPU. This is possible because of a block structure in its system design. Data is sorted and stored in in-memory units called blocks. Unlike other algorithms, this enables the data layout to be reused by subsequent iterations, instead of computing it again. This feature also serves useful for steps like split finding and column sub-sampling\n",
        "\n",
        "**Cache awareness:** In XGBoost, non-continuous memory access is required to get the gradient statistics by row index. Hence, XGBoost has been designed to make optimal use of hardware. This is done by allocating internal buffers in each thread, where the gradient statistics can be stored\n",
        "\n",
        "**Out-of-core computing:** This feature optimizes the available disk space and maximizes its usage when handling huge datasets that do not fit into memory\n",
        "\n",
        "[Source](https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/)"
      ],
      "metadata": {
        "id": "CcQ43rpr-zAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research papers\n",
        "\n",
        "[Paper 1](https://arxiv.org/pdf/2112.02365.pdf)\n",
        "\n",
        "[Paper 2](https://arxiv.org/pdf/2103.06261.pdf)\n",
        "\n",
        "[Tree Based Models](https://github.com/benedekrozemberczki/awesome-decision-tree-papers)"
      ],
      "metadata": {
        "id": "ER4FBitMTaqt"
      }
    }
  ]
}