{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQPsihHtM64/Em11ry8tfm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI-4967-Projects-in-ML-AI/blob/main/Lecture_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projects in ML and AI\n",
        "\n",
        "This course focuses on the fundamentals of Machine Learning and Deep Learning Algorithms specifically when used as problem solving tools. Through the course, students will solve a wide range of problems involving various data types."
      ],
      "metadata": {
        "id": "VDkGLwg2P94-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About the Course\n",
        "\n",
        "1. Syllabus\n",
        "2. Learning outcomes\n",
        "3. Grading\n",
        "4. Emphasis on Practical Applications\n",
        "5. Reading Papers (Discussion Forum)\n",
        "6. Final Project/Paper\n",
        "\n",
        "Pre-requisites (Algorithms, Linear Algebra, Probability theory)\n",
        "Sources for the entire Material:\n",
        "\n",
        "*Neural Networks and Deep Learning (Charu C Aggarwal)*\n",
        "\n",
        "*Dive Into Deep Learning (Online Edition)*"
      ],
      "metadata": {
        "id": "mRZtnJyFUN3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Course Logistics\n",
        "\n",
        "Office Hours: : Tuesday 12pm - 1pm (AE 111) or by appointment (email to: mushtu@rpi.edu).\n",
        "\n",
        "WebEx Link: https://rensselaer.webex.com/meet/mushtu\n",
        "\n",
        "Homeworks/Projects will be due on Fridays.\n",
        "\n",
        "Discussion Forum: A new question/discussion will be posted(every week) on the Submitty Discussion forum. Participation in the discussion counts towards the participation grade."
      ],
      "metadata": {
        "id": "i3lvL6d8iPug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Machine Learning?\n",
        "\n",
        "1.   A machine learning algorithm is the process that uncovers the underlying relationship within the data.\n",
        "\n",
        "2.   The outcome of a machine learning algorithm is called machine learning model, which can be considered as a function , which outputs certain results, when given the input.\n",
        "\n",
        "3.    Rather than a predefined and fixed function, a machine learning model is derived from historical data. Therefore, when fed with different data, the output of machine learning algorithm changes, i.e. the machine learning model changes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-2CcNvIW3rd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Applications\n",
        "\n",
        "\n",
        "Some of the known applications of machine learning are: recognize objects from pictures, predict location of a robot, direct a self-driving car via sensor measurements or chatgpt answering all your queries. These techniques contribute to the new wave of technological advancement often associated with artificial intelligence (AI)."
      ],
      "metadata": {
        "id": "5Rlh_q7oDAmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorizing ML Models\n",
        "\n",
        "### Types of Learning\n",
        "•\tSupervised: Regression, Classification\n",
        "\n",
        "•\tUnsupervised\n",
        "\n",
        "•\tReinforcement Learning\n",
        "\n",
        "### Types of Models\n",
        "\n",
        "•\tDiscriminative\n",
        "\n",
        "•\tGenerative"
      ],
      "metadata": {
        "id": "q_5lV8xF90uY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial Intelligence\n",
        "\n",
        "[AI in everything](https://drive.google.com/file/d/1TpQXNJ0Qv8yt50Yzt8PoVQUVm9-WcjFn/view?usp=sharing)\n",
        "\n",
        "[Article to Read](https://cloud.google.com/learn/artificial-intelligence-vs-machine-learning)"
      ],
      "metadata": {
        "id": "AqLWltVp-LTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Today's Lecture\n",
        "\n",
        "1. Resources\n",
        "2. Grouping of ML Algorithms\n",
        "3. Linear Model\n",
        "4. Regression\n",
        "5. Classification\n",
        "6. Gradient Descent\n"
      ],
      "metadata": {
        "id": "klI7Dre9-UTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources for your Projects:\n",
        "\n",
        "•\tJupyter Notebooks\n",
        "\n",
        "•\tGoogle Colab: https://colab.research.google.com/\n",
        "\n",
        "•\tGitHub: Publish your Notebooks on your GitHub. You can directly publish Colab Notebooks on GitHub\n",
        "\n",
        "•\tShare the link to your project in a .txt file as your Homework Submission\n",
        "\n",
        "## Data for your projects\n",
        "\n",
        "1. https://www.kaggle.com/\n",
        "2. https://www.data.gov/\n",
        "3. https://archive.ics.uci.edu/ml/datasets.php\n",
        "4. https://datasetsearch.research.google.com/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sz-7jj7F-aZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Algorithms’ Grouping\n",
        "\n",
        "*Source for this section: https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms*\n",
        "\n",
        "•\tThe first is a grouping of algorithms by their learning style.\n",
        "\n",
        "\n",
        "\n",
        "• The second grouping is by modeling paradigm.\n",
        "\n",
        "\n",
        "•\tThe third is a grouping of algorithms by their similarity in form or function (like grouping similar animals together)."
      ],
      "metadata": {
        "id": "40dZyiiXqY4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Machine Learning Algorithms Grouped by Style**\n",
        "\n",
        "1. **Supervised Learning Algorithms**\n",
        "\n",
        "  Input data is called training data and has a known label or result such as spam/not-spam or a stock price at a time.\n",
        "\n",
        "  A model is prepared through a training process in which it is required to make predictions and is corrected when those predictions are wrong. The training process continues until the model achieves a desired level of accuracy on the training data.\n",
        "\n",
        "  Example problems are classification and regression.\n",
        "\n",
        "  Example algorithms include: Logistic Regression and the Back Propagation Neural Network."
      ],
      "metadata": {
        "id": "a002Hyq1qkAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Unsupervised Learning Algorithms**\n",
        "\n",
        "  Input data is not labeled and does not have a known result.\n",
        "\n",
        "  A model is prepared by deducing structures present in the input data. This may be to extract general rules. It may be through a mathematical process to systematically reduce redundancy, or it may be to organize data by similarity.\n",
        "\n",
        "  Example problems are clustering, dimensionality reduction and association rule learning.\n",
        "\n",
        "  Example algorithms include: DBScan and K-Means."
      ],
      "metadata": {
        "id": "2hN9h3P8qrcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Semi-Supervised Learning**\n",
        "\n",
        "  Input data is a mixture of labeled and unlabelled examples.\n",
        "\n",
        "  There is a desired prediction problem but the model must learn the structures to organize the data as well as make predictions.\n",
        "\n",
        "  Example problems are classification and regression.\n",
        "\n",
        "  Example algorithms are extensions to other flexible methods that make assumptions about how to model the unlabeled data."
      ],
      "metadata": {
        "id": "eApZl1-TqypW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning**\n",
        "\n",
        "Reinforcement learning is the training of machine learning models to make a sequence of decisions.\n",
        "The agent learns to achieve a goal in an uncertain, potentially complex environment.\n",
        "In reinforcement learning, an artificial intelligence faces a game-like situation. The computer employs trial and error to come up with a solution to the problem.\n",
        "\n",
        "To get the machine to do what the programmer wants, the artificial intelligence gets either rewards or penalties for the actions it performs. Its goal is to maximize the total reward.\n",
        "\n",
        "[Article Link](https://www.geeksforgeeks.org/what-is-reinforcement-learning/)"
      ],
      "metadata": {
        "id": "m9rTRCKiq33p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Machine Learning Algorithms Grouped by Modeling Paradigm**\n",
        "\n",
        "The relationship between data and outcome can be mathematically modelled in two different ways: Discriminative and Generative models.\n",
        "\n",
        "1. Discriminative Models: These models make predictions on unseen data by considering conditional probability. A model of the form P(y|x) is a discriminative classifier since it can be used to discriminate between various classes.\n",
        "\n",
        "2. Generative Models: These models focus on the distribution of the dataset and determine how likely a given sample is. These models capture the joint probability of the form P(y,x) or P(x) if there are no labels.\n",
        "\n",
        "Source: [Link](https://developers.google.com/machine-learning/gan)"
      ],
      "metadata": {
        "id": "YKA8N5Vkq7vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem formulation\n",
        "\n",
        "Consider a classification problem where the task at hand is to decide whether an email is a spam or ham. Suppose we have labeled data:\n",
        "\n",
        "Labels: y\n",
        "\n",
        "Features: x = {x1,x2,x3....xn}\n",
        "\n",
        "The goal is to determine probability of spam email i.e. P(y=1|x).\n",
        "We can solve this problem using either the discriminative approach or generative approach.\n",
        "\n",
        "**Discriminative models**: These models will assume some underlying functional form for P(y|x). The parameters of the model are then determined by using training data.\n",
        "\n",
        "**Generative models**: Under these models, the conditional probability P(y|x) is calculated by using the prior P(y) and the likelihood P(x|y) from the training data. Next, Bayes theorem is applied to get the final result.\n",
        "\n",
        "\n",
        "[Further Reading](https://www.analyticsvidhya.com/blog/2021/07/deep-understanding-of-discriminative-and-generative-models-in-machine-learning/#:~:text=In%20simple%20words%2C%20a%20discriminative,probability%20for%20a%20given%20example.)"
      ],
      "metadata": {
        "id": "2T95tVyMA-Qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Algorithms Grouped by Similarity**\n",
        "\n",
        "1. **Regression Algorithms**: The most popular regression algorithms are:\n",
        "\n",
        "  Ordinary Least Squares Regression (OLSR)\n",
        "\n",
        "  Linear Regression\n",
        "\n",
        "  Logistic Regression\n",
        "\n",
        "  Stepwise Regression\n",
        "\n",
        "2. **Instance-based Algorithms**\n",
        "    Instance-based learning model is a decision problem with instances or examples of training data that are deemed important or required to the model. The most popular instance-based algorithms are:\n",
        "\n",
        "  k-Nearest Neighbor (kNN)\n",
        "\n",
        "  Self-Organizing Map (SOM)\n",
        "\n",
        "  Support Vector Machines (SVM)\n",
        "\n",
        "3. **Regularization Algorithms**\n",
        "\n",
        "  An extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing.\n",
        "  The most popular regularization algorithms are:\n",
        "\n",
        "  Ridge Regression\n",
        "\n",
        "  Least Absolute Shrinkage and Selection Operator (LASSO)\n",
        "\n",
        "  Elastic Net\n",
        "\n",
        "  Least-Angle Regression (LARS)\n",
        "\n",
        "4. **Decision Tree Algorithms**\n",
        "\n",
        "  Decision tree methods construct a model of decisions made based on actual values of attributes in the data.\n",
        "\n",
        "  The most popular decision tree algorithms are:\n",
        "\n",
        "  Classification and Regression Tree (CART)\n",
        "\n",
        "  Conditional Decision Trees\n",
        "\n",
        "5. **Clustering Algorithms**\n",
        "\n",
        "  Clustering methods are typically organized by the modeling approaches such as centroid-based and hierarchal.\n",
        "\n",
        "  The most popular clustering algorithms are:\n",
        "\n",
        "  k-Means\n",
        "\n",
        "  k-Medians\n",
        "\n",
        "  Expectation Maximisation (EM)\n",
        "  \n",
        "  Heirarchical Clustering\n",
        "  \n",
        "  DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
        "\n",
        "\n",
        "6. **Artificial Neural Network Algorithms**\n",
        "\n",
        "  The most popular artificial neural network algorithms are:\n",
        "\n",
        "  Perceptron\n",
        "\n",
        "  Multilayer Perceptrons (MLP)\n",
        "\n",
        "  Back-Propagation\n",
        "\n",
        "  Stochastic Gradient Descent\n",
        "\n",
        "  Hopfield Network\n",
        "\n",
        "  Radial Basis Function Network (RBFN)\n",
        "\n",
        "7. **Deep Learning Algorithms**\n",
        "\n",
        "  Modern update to Artificial Neural Networks that exploit abundant cheap computation.\n",
        "  The most popular deep learning algorithms are:\n",
        "\n",
        "  Convolutional Neural Network (CNN)\n",
        "\n",
        "  Recurrent Neural Networks (RNNs)\n",
        "\n",
        "  Long Short-Term Memory Networks (LSTMs)\n",
        "\n",
        "  Stacked Auto-Encoders\n",
        "\n",
        "  Deep Boltzmann Machine (DBM)\n",
        "  \n",
        "  Deep Belief Networks (DBN)\n"
      ],
      "metadata": {
        "id": "zcDctCtJZET8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Linear Model\n",
        "\n",
        "In a linear model, we assume there exists a linear relationship between the indepenedent and dependent variables. We will discuss the linear model from the perspective of Regression (a technique used to predict numeric values mostly).\n",
        "\n",
        "## Linear Regression\n",
        "\n",
        "Linear Regression is one of the most basic tools available for regression. We assume that the relationship between the independent variables (denoted by $x$) and the dependent variable (denoted by $y$) is linear. Therefore, $y$ can be expressed as a weighted sum of the elements in $x$.Some noise is permissible on the observations under the assumption that its well-behaved(Gaussian Distribution).\n",
        "\n",
        "**Notation and terms used:** Let us consider an example of predicting House prices based on their area(square foot) and age(in years).\n",
        " In the language of Machine Learning, the dataset is called training data or training set (each row is called an example or datapoint). The value we are trying to predict is the label(target). Within the dataset, the independent variables upon which predictions are based are called features (covariates).\n",
        "\n",
        " Usually we use $n$ to denote the number of examples (observations in our dataset). We index the data examples by $i$ such that each input is\n",
        " $x^{(i)}$ = $[x_1^{(i)},x_2^{(i)}]$ and the corresponding labels as $y^{(i)}$.\n",
        "\n",
        " **The Model**: The linearity assumption states that the target variable(price) can be expressed as a weighted sum of the features:\n",
        "\n",
        "$price = w_{area}.area + w_{age}.age +b$\n",
        "\n",
        "Given a dataset, our goal is to choose the weights  w  and the bias  b  such that on average, the predictions made according to our model best fit the true prices observed in the data. Models whose output prediction is determined by the affine transformation of input features are linear models, where the affine transformation is specified by the chosen weights and bias.\n"
      ],
      "metadata": {
        "id": "4aA91W6EKBgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function\n",
        "\n",
        "In any ML problem, we have a target value and a predicted value.The loss function is a way to quantify the difference between the real and predicted value of the target. The loss will usually be a non-negative number where smaller values are better. The most popular loss function in regression problems is the squared error.\n",
        "\n",
        "Let the predicted value for an example $i$ be $\\hat{y}^{(i)}$ and the corresponding actual label be ${y}^{(i)}$, then the loss function (squared error) is given by:\n",
        "\n",
        "$l^{i}(w,b)$ = $\\frac{1}{2}(\\hat{y}^{(i)} - y^{(i)})^{2}$\n",
        "\n",
        "The constant  1/2  makes no real difference but will prove notationally convenient, canceling out when we take the derivative of the loss. Since the training dataset is given to us, and thus out of our control, the empirical error is only a function of the model parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "WjQKTBbGKLlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To measure the quality of a model on the entire dataset of  $n $ examples, we simply average (or equivalently, sum) the losses on the training set. We now get something known as the cost function:\n",
        "\n",
        "$L(w,b) = \\frac{1}{n} \\sum_{i=1}^{n} l_i(w,b)$\n",
        "\n",
        "When training the model, we want to find parameters ( $w^∗$,$b^∗$ ) that minimize the total loss across all training examples:\n",
        "\n",
        "$w^∗$,$b^∗$ = $argmin_{w,b} \\space L(w,b) $\n"
      ],
      "metadata": {
        "id": "WKf4UjCdKQo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This optimization problem is solved analytically by applying a simple formula. We can assume the bias  $b$  to be a part of  $w$  by appending a column to the design matrix consisting of all ones. Then our prediction problem is to minimize  $∥y−Xw∥^2$ . There is just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain. Taking the derivative of the loss with respect to  w  and setting it equal to zero yields the analytic (closed-form) solution:\n",
        "\n",
        "$\\textbf{w}^* = (X^TX)^{-1} X^Ty$"
      ],
      "metadata": {
        "id": "1_9X6pGDKXZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Model Assumptions\n",
        "\n",
        "1. Linearity: The relationship between the dependent and independent variables is linear.\n",
        "\n",
        "2. Independence: The observations are independent of each other.\n",
        "\n",
        "3. Homoscedasticity: The variance of the errors is constant across all levels of the independent variables.\n",
        "\n",
        "4. Normality: The errors follow a normal distribution.\n",
        "\n",
        "5. No multicollinearity: The independent variables are not highly correlated with each other.\n",
        "\n",
        "6. No endogeneity: There is no relationship between the errors and the independent variables."
      ],
      "metadata": {
        "id": "DQEfattpKb9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLE (Maximum Likelihood estimation)\n",
        "\n",
        "In general most ML models are formulated as an optimization problem (minimization mostly) and the ML algorithm(for example gradient descent) aims to minimize this function given some data. MLE is a recipe for formulating the loss function that is to be minimized. It involves finding the parameter values that maximize the likelihood function\n",
        ". The likelihood function measures the probability of observing the given data under the assumed model.\n",
        "\n",
        "The most common approach to parameter estimation is to pick the parameters that assign the highest\n",
        "probability to the training data; this is called maximum likelihood estimation or MLE.\n",
        "\n",
        "[Maximum Likelihood estimation](https://medium.com/analytics-vidhya/mse-vs-mle-for-linear-regression-f4ce3f6b990e#:~:text=What%20is%20MSE%20%3F,-Mean%20squared%20error&text=What%20is%20MLE%20%3F&text=Maximum%20likelihood%20estimation%20(MLE)%20is,observed%20data%20is%20most%20probable.)"
      ],
      "metadata": {
        "id": "98Qzf-HzKfob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Binary classification tasks involve predicting categorical outputs. For example Yes/No, Spam/Ham, 0/1 etc. If we use linear regression to predict whether an email is spam or not, we will get a continuous outcome. For bounded target values, specifically with a binary outcome logistic regression is used.\n",
        "\n",
        "For predicting a binary outcome we use an activation on top of our linear model(from linear regression). Usually the sigmoid function is used for activation in a bianry classification problem.\n",
        "\n",
        "\n",
        "[Capture2.PNG](https://drive.google.com/file/d/12WyuqEuA4TApXyIyJyiyvACpIHy8VwYF/view?usp=sharing)\n",
        "\n",
        "The predicted outcome:\n",
        "\n",
        "$\\hat{y}$= $\\sigma(\\textbf{w}^Tx + b)$\n",
        "\n",
        "where $\\sigma(z) = \\frac{1}{(1+e^{-z})}$"
      ],
      "metadata": {
        "id": "MOmOhFHyKkJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function for Logistic Regression\n",
        "\n",
        "We cannot use the squared error loss function for logistic regression. Can you think why?\n",
        "\n",
        "For a given example $i$, the loss function for a single instance is given by:\n",
        "\n",
        "$l^{i}(y^{(i)},\\hat{y}^{(i)})$ = $-(y^{(i)}\\space log\\hat{y}^{(i)} + (1-y^{i}) log(1-\\hat{y}^{(i)}))$\n",
        "\n",
        "Cost function for the entire data:\n",
        "\n",
        "$L(y,\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} l^{i}(y^{(i)},\\hat{y}^{(i)})$\n",
        "\n",
        "[Extra Resource](https://www.analyticsvidhya.com/blog/2020/11/binary-cross-entropy-aka-log-loss-the-cost-function-used-in-logistic-regression/)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rrR505-BK63c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "In any Machine Learning Algorithm, our goal is to minimize the Cost function. In most cases, the optimization problem cannot be solved analytically. For these cases the technique utilized is that of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the cost function. This algorithm is called **gradient descent**.\n",
        "\n",
        "One basic approach of gradient descent consists of taking the derivative of the cost function, which is an average of the losses computed on every single example in the dataset. In practice, this can be extremely slow: we must pass over the entire dataset before making a single update. Thus, we will often settle for sampling a random minibatch of examples every time we need to compute the update, a variant called *minibatch stochastic gradient descent*.\n",
        "\n",
        "Steps to Gradient Descent:\n",
        "1. Initialize the values of the model parameters, typically at random;\n",
        "2. Iteratively sample random minibatches from the data, updating the parameters in the direction of the negative gradient\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "$(w,b) \\leftarrow (w, b)- \\frac{\\eta}{B}\\sum_{i\\epsilon{B}}\\partial_{(w,b)}l^{i}(y^{(i)},\\hat{y}^{(i)})$\n",
        "\n",
        "Here $B$ represents the number of examples in each minibatch (the batch size) and  $\\eta$  denotes the learning rate. The values of the batch size and learning rate are input paramters and not typically learned through model training.\n",
        "\n",
        "These parameters that are tunable but not updated in the training loop are called hyperparameters. Hyperparameter tuning is the process by which hyperparameters are chosen, and typically requires that we adjust them based on the results of the training loop as assessed on a separate validation dataset (or validation set).\n",
        "\n",
        "After training for some predetermined number of iterations (or until some other stopping criteria are met), we record the estimated model parameters, denoted  $\\hat{w},\\hat{b}$ . In most cases these parameters will not be the exact minimizers of the loss because, although the algorithm converges slowly towards the minimizers it cannot achieve it exactly in a finite number of steps.\n",
        "\n",
        "Once we have these learned paramters, we can go back to our problem and calculate the predicted value for each example."
      ],
      "metadata": {
        "id": "7EGFAiEjLL9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Readings for this week\n",
        "\n",
        "[Paper 1](https://www.researchgate.net/profile/Joanne-Peng-4/publication/242579096_An_Introduction_to_Logistic_Regression_Analysis_and_Reporting/links/0deec5374c228b7fa1000000/An-Introduction-to-Logistic-Regression-Analysis-and-Reporting.pdf)\n",
        "\n",
        "[Paper 2](https://www.nottingham.ac.uk/mathematics/documents/pgt-course-resources/linear-models.pdf)\n"
      ],
      "metadata": {
        "id": "3rV8tAMCLVsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "[numpy](https://numpy.org/)\n",
        "\n",
        "[pandas](https://drive.google.com/file/d/1s-yE1CguMH6_2mG4PiAjk8zjp75t8eEm/view?usp=sharing)"
      ],
      "metadata": {
        "id": "iqL_-uCVLeX7"
      }
    }
  ]
}