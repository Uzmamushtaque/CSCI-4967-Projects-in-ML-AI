{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWFhNqpJOEZm1MtQdsCmOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzmamushtaque/CSCI-4967-Projects-in-ML-AI/blob/main/Lecture_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 10\n",
        "\n",
        "## Today's Topics\n",
        "\n",
        "1. Applications of Reccurent Neural Networks (RNNs)\n",
        "2. Pretrained Embeddings in Sequential Data (Word2Vec)\n",
        "\n"
      ],
      "metadata": {
        "id": "4vapw-1QHL39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applications in ML\n",
        "\n",
        "RNNs find numerous applications in ML and AI, most predominant being information retrieval, speech recognition, handwriting/text recognition, computational biology etc. Most of these application fall under two categories:\n",
        "\n",
        "1. Conditional Language Modeling: There can be a task that requires some textual outcome conditioned on some input given in image or text or any other form. For example in machine translation the input is text from a source language and the RNN converts that to the target language. In image captioning the input is an image and the output is a piece of text generated by the RNN.\n",
        "\n",
        "2. Leveraging Token specific outputs: Every entity in a sequence can be considered to carry some information which is referred to as a token. For example in sentence level classification, the end of sentence is the token after which some sentiment of the sentence is predicted. In speech recongnition different time stamps correspond to tokens"
      ],
      "metadata": {
        "id": "ktCXXyacJ5KN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Image Captioning\n",
        "\n",
        "Image captioning is the task of describing the content of an image in words. In general it lies at the intersection of Convolution neural networks (CNNs) and RNNs. It involves an encoder-decoder architecture, where the encoder is a CNN which learns the representaion of an image and the associated caption is considered to be the label. Initialization is usually done for both networks in isolation but the joint training must be done to for image-caption pair for final weight updates.\n",
        "\n",
        "Research Directions: Exploring different architectures in the encoder network or the decoder network (LSTMs are a good choice).\n",
        "\n",
        "Fine Tuning an Image captioning model.[Link](https://huggingface.co/docs/transformers/main/en/tasks/image_captioning)\n",
        "\n",
        "[Research](https://paperswithcode.com/task/image-captioning)"
      ],
      "metadata": {
        "id": "3yHYyM3JMsxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Translation\n",
        "\n",
        "Sequence to sequence translation in general refers to problems that can be solved with 2 RNNs (or it variants) tied end to end.\n",
        "\n",
        "Research Areas:\n",
        "\n",
        "* Speech Recognition\n",
        "\n",
        "* Name Entity/Subject Extraction to identify the main subject from a body of text\n",
        "\n",
        "* Relation Classification to tag relationships between various entities tagged in the above step\n",
        "\n",
        "* Chatbot skills to have conversational ability and engage with customers\n",
        "\n",
        "* Text Summarization to generate a concise summary of a large amount of text\n",
        "\n",
        "* Question Answering systems\n",
        "\n",
        "Important issue with most machine translation tasks are that these are computationally expensive.\n",
        "\n",
        "Machine translation evaluation metrics score machine-translated segments based on their similarity to reference translations. Here are some evaluation metrics for machine translation:\n",
        "\n",
        "BLEU (BiLingual Evaluation Understudy): A metric that automatically evaluates machine-translated text. BLEU scores range from zero to one and measure the similarity of the machine-translated text to a set of high quality reference translations.\n",
        "\n",
        "ROUGE: A metric used to assess the quality of text summarization tasks.\n",
        "\n",
        "Edit distance: The number of modifications a human editor is required to make to a system translation to contain the complete meaning in easily understandable English.\n",
        "\n",
        "[Reading](https://towardsdatascience.com/neural-machine-translation-15ecf6b0b)\n",
        "\n",
        "[Paper](https://arxiv.org/pdf/1508.04025.pdf)"
      ],
      "metadata": {
        "id": "1KcuxU85RS4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Level Classification\n",
        "\n",
        "Sentence level classification is done for sentiment analysis. A basic many-to-one RNN is used for sentence classification.\n",
        "\n",
        "Sophisticated bidirectional models like BERT are now used for this task but the basic architecture is that of a Bidirectional RNN.\n",
        "[Link](https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_recurrent-modern/bi-rnn.ipynb)"
      ],
      "metadata": {
        "id": "NdTXWhnPWLEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Series Forecasting\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are deep learning models that can be utilized for time series analysis, with recurrent connections that allow them to retain information from previous time steps. Popular variants include Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), which can learn long-term dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "xhDjakW_Y3n6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings\n",
        "\n",
        "Learning word embeddings (representation of text) is an important task in NLP. Historically singualr value decomposition is used to create embeddings in text data. This technique was more suited for document level embeddings and is not suited for ordered representations. The family of word2vec methods is suited for creating word embeddings.\n",
        "\n",
        "Two variants of word2vec are:\n",
        "\n",
        "1. Predicting target words from context: Predict the ith word in a given sequence of i-1 words. This is also called the CBOW or continuous bag of words model (CBOW).\n",
        "\n",
        "2. Predicting context from target words: This model predicts the context (set of words) around the ith word. This is the skip-grams model. Two techniques fall under this category, one is multinomial which predicts one word out of d outcomes. Second is Bernoulli model which predicts whether a particula context is present for a word or not."
      ],
      "metadata": {
        "id": "vOomztamZa9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given enough data, usage and contexts, Word2vec can make highly accurate guesses about a word’s meaning based on past appearances. Those guesses can be used to establish a word’s association with other words (e.g. “man” is to “boy” what “woman” is to “girl”), or cluster documents and classify them by topic. Those clusters can form the basis of search, sentiment analysis and recommendations in such diverse fields as scientific research, legal discovery, e-commerce and customer relationship management.\n",
        "\n",
        "The output of the Word2vec neural net is a vocabulary in which each item has a vector attached to it, which can be fed into a deep-learning net or simply queried to detect relationships between words.\n",
        "\n",
        "[Source](https://wiki.pathmind.com/word2vec)\n",
        "\n",
        "[Reading](https://www.analyticsvidhya.com/blog/2021/07/word2vec-for-word-embeddings-a-beginners-guide/)"
      ],
      "metadata": {
        "id": "GgvT9JmFjIJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Readings\n",
        "[Paper](https://arxiv.org/pdf/1301.3781.pdf)"
      ],
      "metadata": {
        "id": "RT51cZEyjx0e"
      }
    }
  ]
}